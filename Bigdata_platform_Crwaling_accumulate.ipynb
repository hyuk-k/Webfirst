{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhnURcURkKNXk+INqjxY2F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyuk-k/Webfirst/blob/main/Bigdata_platform_Crwaling_accumulate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3sw9tbrJePd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 파일명을 정의합니다. 누적 데이터를 저장할 파일입니다.\n",
        "accumulated_file_name = 'accumulated_crawling_results.csv'\n",
        "\n",
        "# 누적 데이터 파일이 이미 존재하면 불러옵니다.\n",
        "if os.path.exists(accumulated_file_name):\n",
        "    accumulated_data = pd.read_csv(accumulated_file_name, encoding='utf-8-sig')\n",
        "else:\n",
        "    # 누적 데이터 파일이 존재하지 않을 때, 적절한 열을 가진 빈 데이터프레임을 생성합니다.\n",
        "    accumulated_data = pd.DataFrame(columns=['구분', '데이터명', '사이트 주소', '날짜', '다운로드 횟수'])\n",
        "\n",
        "# 각 제목을 구분과 데이터명으로 나누기 위한 함수\n",
        "def split_title(title):\n",
        "    parts = title.split(' - ')\n",
        "    if len(parts) > 1:\n",
        "        return parts[0].strip(), parts[1].strip()\n",
        "    else:\n",
        "        return title.strip(), ''\n",
        "\n",
        "# 각 페이지에서 날짜별 다운로드 수를 크롤링하는 함수\n",
        "def fetch_data(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # 웹 페이지에서 제목을 추출하는 부분\n",
        "    title_element = soup.find('div', class_='data_market_title')\n",
        "    title = title_element.text.strip() if title_element else '제목을 찾을 수 없습니다'\n",
        "    title = re.sub(r'^\\d+\\.\\s+', '', title)  # 제목에서 숫자와 점을 제거\n",
        "\n",
        "    # 'data_market_info' 클래스를 가진 ul 태그를 찾고, 그 안의 모든 li 태그를 찾습니다.\n",
        "    data_info = soup.find('ul', class_='data_market_info').find_all('li')\n",
        "\n",
        "    # 날짜별 다운로드 수를 추출하는 부분\n",
        "    download_counts = {}\n",
        "\n",
        "   # 현재 날짜와 시간 추출\n",
        "    now = datetime.now()\n",
        "    date_str = now.strftime('%Y-%m-%d %H:%M:%S')  # '년-월-일 시:분:초' 형식으로 형식화합니다.\n",
        "\n",
        "    # '다운로드' 횟수 추출\n",
        "    for li in data_info:\n",
        "        if '다운로드' in li.text:\n",
        "            count = li.find('b', class_='text-bold').text.strip()\n",
        "            download_counts[date_str] = count  # 현재 날짜와 시간을 사용하여 딕셔너리에 추가합니다.\n",
        "            break\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'download_counts': download_counts\n",
        "    }\n",
        "\n",
        "# URL 목록\n",
        "urls = [\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=21\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=207\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=17\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=18\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=19\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=20\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=22\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=16\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=240\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=241\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=728\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=730\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=731\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=732\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=733\",\n",
        "    \"https://bigdata-geo.kr/user/dataset/view.do?data_sn=729\"\n",
        "]\n",
        "\n",
        "# 크롤링 결과 저장 리스트\n",
        "crawling_results = []\n",
        "\n",
        "# # 각 URL 크롤링\n",
        "# for url in urls:\n",
        "#     data = fetch_data(url)\n",
        "#     title = data['title']\n",
        "#     download_counts = data['download_counts']\n",
        "#     category, data_name = split_title(title)\n",
        "#     # 각 날짜별 다운로드 횟수와 함께 결과에 추가합니다.\n",
        "#     for date, count in download_counts.items():\n",
        "#         crawling_results.append((category, data_name, url, date, count))\n",
        "\n",
        "# 각 URL 크롤링\n",
        "for url in urls:\n",
        "    data = fetch_data(url)\n",
        "    title = data['title']\n",
        "    download_counts = data['download_counts']\n",
        "    category, data_name = split_title(title)\n",
        "\n",
        "    # 날짜별로 데이터를 누적합니다.\n",
        "    for date, count in download_counts.items():\n",
        "        # 날짜와 카테고리, 데이터명을 기준으로 기존 데이터를 확인합니다.\n",
        "        existing_data = accumulated_data[\n",
        "            (accumulated_data['날짜'] == date) &\n",
        "            (accumulated_data['구분'] == category) &\n",
        "            (accumulated_data['데이터명'] == data_name)\n",
        "        ]\n",
        "\n",
        "        # 기존 데이터가 있다면, 다운로드 횟수를 업데이트합니다.\n",
        "        if not existing_data.empty:\n",
        "            accumulated_data.loc[existing_data.index, '다운로드 횟수'] = count\n",
        "        else:\n",
        "            # 새로운 데이터를 추가합니다.\n",
        "            new_data = pd.DataFrame({\n",
        "                '구분': [category],\n",
        "                '데이터명': [data_name],\n",
        "                '사이트 주소': [url],\n",
        "                '날짜': [date],\n",
        "                '다운로드 횟수': [count]\n",
        "            })\n",
        "            accumulated_data = pd.concat([accumulated_data, new_data], ignore_index=True)\n",
        "\n",
        "# 누락된 값에 대한 처리 (NaN 값을 '-'로 변경)\n",
        "accumulated_data.fillna('-', inplace=True)\n",
        "\n",
        "# 데이터프레임 옵션 설정\n",
        "pd.set_option('display.unicode.east_asian_width', True)\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df_accumulated_data = pd.DataFrame(accumulated_data, columns=['구분', '데이터명', '사이트 주소', '날짜', '다운로드 횟수'])\n",
        "\n",
        "# '연번' 칼럼 추가: 1부터 시작하며 각 행마다 1씩 증가\n",
        "df_accumulated_data['연번'] = range(1, len(df_accumulated_data) + 1)\n",
        "\n",
        "# '연번' 칼럼을 첫 번째 칼럼으로 이동\n",
        "cols = df_accumulated_data.columns.tolist()\n",
        "# '연번' 칼럼의 인덱스를 찾아 첫 번째 위치로 이동\n",
        "cols = cols[-1:] + cols[:-1]\n",
        "df_accumulated_data = df_accumulated_data[cols]\n",
        "\n",
        "# 누적된 데이터프레임을 출력합니다.\n",
        "print(df_accumulated_data)\n",
        "\n",
        "# 데이터프레임을 CSV 파일로 저장합니다.\n",
        "df_accumulated_data.to_csv(accumulated_file_name, index=False, encoding='utf-8-sig')\n",
        "\n",
        "#####################################################################################\n",
        "## MYSQL 적재\n",
        "from sqlalchemy import create_engine\n",
        "import pandas as pd\n",
        "\n",
        "# 데이터베이스 연결 정보 설정\n",
        "username = 'root'  # MySQL 사용자 이름\n",
        "password = '0928'  # MySQL 비밀번호\n",
        "host = 'localhost'          # MySQL 호스트\n",
        "dbname = 'test01'      # 데이터베이스 이름\n",
        "table_name = 'crwaling02'   # 데이터를 적재할 테이블 이름\n",
        "\n",
        "# SQLAlchemy 엔진 생성\n",
        "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}/{dbname}')\n",
        "\n",
        "# # 데이터베이스에 적재할 DataFrame 컬럼 순서 조정\n",
        "# df_accumulated_data = df_accumulated_data[['연번', '구분', '데이터명', '사이트 주소', '날짜', '다운로드 횟수']]\n",
        "\n",
        "# DataFrame을 MySQL 테이블에 적재합니다.\n",
        "# if_exists='replace'를 사용하면 기존 테이블을 새로운 데이터로 대체합니다.\n",
        "df_accumulated_data.to_sql(name=table_name, con=engine, if_exists='append', index=False)"
      ]
    }
  ]
}